{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8a2aab",
   "metadata": {},
   "source": [
    "# Neural Network Model - Implementation\n",
    "\n",
    "## Features\n",
    "\n",
    "- **NeuralNetworkModel**: MLP regressor with automatic scaling\n",
    "- **NeuralNetworkGoalPredictor**: Dual NN for home/away goals\n",
    "- Training loss curve visualization\n",
    "- Early stopping for regularization\n",
    "- Architecture configuration\n",
    "\n",
    "## Critical: Feature Scaling\n",
    "\n",
    "⚠️ **Neural networks REQUIRE feature scaling!** Unlike tree-based models, NNs are very sensitive to feature magnitudes. This notebook handles scaling automatically.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "| Parameter | Default | Range | Impact |\n",
    "|-----------|---------|-------|--------|\n",
    "| hidden_layer_sizes | (64, 32) | Various | Architecture depth/width |\n",
    "| activation | 'relu' | relu/tanh | Non-linearity type |\n",
    "| alpha | 0.001 | 0.0001-0.1 | L2 regularization |\n",
    "| learning_rate_init | 0.001 | 0.0001-0.01 | Step size |\n",
    "| max_iter | 500 | 100-2000 | Training epochs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b243f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Neural Network ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd8a40",
   "metadata": {},
   "source": [
    "## NeuralNetworkModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel:\n",
    "    \"\"\"\n",
    "    Neural network with automatic feature scaling.\n",
    "    \n",
    "    CRITICAL: This class handles scaling automatically because\n",
    "    neural networks require normalized features for proper training.\n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULT_PARAMS = {\n",
    "        'hidden_layer_sizes': (64, 32),\n",
    "        'activation': 'relu',\n",
    "        'solver': 'adam',\n",
    "        'alpha': 0.001,\n",
    "        'learning_rate': 'adaptive',\n",
    "        'learning_rate_init': 0.001,\n",
    "        'max_iter': 500,\n",
    "        'early_stopping': True,\n",
    "        'validation_fraction': 0.1,\n",
    "        'n_iter_no_change': 20,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, params=None, scaler_type='standard'):\n",
    "        self.params = {**self.DEFAULT_PARAMS, **(params or {})}\n",
    "        \n",
    "        # Create scaler - REQUIRED for neural networks\n",
    "        if scaler_type == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif scaler_type == 'robust':\n",
    "            self.scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scaler: {scaler_type}\")\n",
    "        \n",
    "        self.model = MLPRegressor(**self.params)\n",
    "        self.feature_names = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit with automatic scaling.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names = list(X.columns)\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Train\n",
    "        self.model.fit(X_scaled, y)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        print(f\"Converged in {self.model.n_iter_} iterations\")\n",
    "        if hasattr(self.model, 'best_loss_'):\n",
    "            print(f\"Best loss: {self.model.best_loss_:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict with automatic scaling.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Scale using fitted scaler\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate model performance.\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        \n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y, predictions)),\n",
    "            'mae': mean_absolute_error(y, predictions),\n",
    "            'r2': r2_score(y, predictions),\n",
    "        }\n",
    "    \n",
    "    def get_loss_curve(self):\n",
    "        \"\"\"Get training loss curve.\"\"\"\n",
    "        return self.model.loss_curve_ if hasattr(self.model, 'loss_curve_') else None\n",
    "    \n",
    "    def get_architecture(self):\n",
    "        \"\"\"Get network architecture summary.\"\"\"\n",
    "        return {\n",
    "            'layers': len(self.params['hidden_layer_sizes']),\n",
    "            'hidden_layer_sizes': self.params['hidden_layer_sizes'],\n",
    "            'total_neurons': sum(self.params['hidden_layer_sizes']),\n",
    "            'activation': self.params['activation'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224e9bf",
   "metadata": {},
   "source": [
    "## Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hockey_data(n_games=1000):\n",
    "    \"\"\"Generate synthetic hockey data with realistic features.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = {\n",
    "        'home_elo': np.random.normal(1500, 100, n_games),\n",
    "        'away_elo': np.random.normal(1500, 100, n_games),\n",
    "        'home_goals_avg': np.random.uniform(2.5, 3.5, n_games),\n",
    "        'away_goals_avg': np.random.uniform(2.5, 3.5, n_games),\n",
    "        'home_goals_against_avg': np.random.uniform(2.5, 3.5, n_games),\n",
    "        'away_goals_against_avg': np.random.uniform(2.5, 3.5, n_games),\n",
    "        'home_pp_pct': np.random.uniform(0.15, 0.30, n_games),\n",
    "        'away_pp_pct': np.random.uniform(0.15, 0.30, n_games),\n",
    "        'home_pk_pct': np.random.uniform(0.75, 0.90, n_games),\n",
    "        'away_pk_pct': np.random.uniform(0.75, 0.90, n_games),\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['elo_diff'] = df['home_elo'] - df['away_elo']\n",
    "    \n",
    "    # Generate realistic goals\n",
    "    home_base = 3.0 + 0.001 * df['elo_diff'] + 0.3 * (df['home_goals_avg'] - 3.0)\n",
    "    away_base = 3.0 - 0.001 * df['elo_diff'] + 0.3 * (df['away_goals_avg'] - 3.0)\n",
    "    \n",
    "    df['home_goals'] = np.random.poisson(np.maximum(home_base, 1.5))\n",
    "    df['away_goals'] = np.random.poisson(np.maximum(away_base, 1.5))\n",
    "    \n",
    "    return df\n",
    "\n",
    "games_df = generate_hockey_data(1000)\n",
    "print(f\"Generated {len(games_df)} games\")\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = [c for c in games_df.columns if c not in ['home_goals', 'away_goals']]\n",
    "X = games_df[feature_cols]\n",
    "y = games_df['home_goals']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1a375",
   "metadata": {},
   "source": [
    "## Why Scaling Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature value ranges\n",
    "print(\"Feature Value Ranges (unscaled):\")\n",
    "print(X.describe().loc[['min', 'max', 'mean', 'std']].T)\n",
    "\n",
    "print(\"\\n⚠️ Notice: ELO values (~1500) are 1000x larger than percentages (~0.2)\")\n",
    "print(\"Without scaling, the network would be dominated by large-magnitude features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scaled vs unscaled training\n",
    "print(\"Training WITHOUT scaling (will likely fail or converge poorly):\")\n",
    "try:\n",
    "    unscaled_model = MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        max_iter=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    unscaled_model.fit(X_train.values, y_train.values)\n",
    "    print(f\"  Iterations: {unscaled_model.n_iter_}\")\n",
    "    pred = unscaled_model.predict(X_test.values)\n",
    "    print(f\"  Test RMSE: {np.sqrt(mean_squared_error(y_test, pred)):.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "print(\"\\nTraining WITH scaling (proper approach):\")\n",
    "scaled_model = NeuralNetworkModel({'max_iter': 100})\n",
    "scaled_model.fit(X_train, y_train)\n",
    "metrics = scaled_model.evaluate(X_test, y_test)\n",
    "print(f\"  Test RMSE: {metrics['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f368914",
   "metadata": {},
   "source": [
    "## Train Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f9630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with default parameters\n",
    "nn_model = NeuralNetworkModel()\n",
    "nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Print architecture\n",
    "arch = nn_model.get_architecture()\n",
    "print(f\"\\nArchitecture: {arch['hidden_layer_sizes']}\")\n",
    "print(f\"Total hidden neurons: {arch['total_neurons']}\")\n",
    "print(f\"Activation: {arch['activation']}\")\n",
    "\n",
    "# Evaluate\n",
    "train_metrics = nn_model.evaluate(X_train, y_train)\n",
    "test_metrics = nn_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for k, v in train_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6eb0dd",
   "metadata": {},
   "source": [
    "## Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "loss_curve = nn_model.get_loss_curve()\n",
    "\n",
    "if loss_curve:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_curve, color='steelblue', linewidth=2)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark early stopping point\n",
    "    if nn_model.params['early_stopping']:\n",
    "        plt.axvline(x=len(loss_curve)-1, color='red', linestyle='--', \n",
    "                    label=f'Early stop @ iter {len(loss_curve)}')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No loss curve available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde5806",
   "metadata": {},
   "source": [
    "## Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different architectures\n",
    "architectures = [\n",
    "    (32,),               # Shallow, narrow\n",
    "    (64,),               # Single layer\n",
    "    (64, 32),            # Two layers\n",
    "    (128, 64),           # Wider\n",
    "    (128, 64, 32),       # Three layers\n",
    "    (256, 128, 64),      # Deep and wide\n",
    "]\n",
    "\n",
    "results = []\n",
    "for arch in architectures:\n",
    "    model = NeuralNetworkModel({'hidden_layer_sizes': arch, 'max_iter': 200})\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_m = model.evaluate(X_train, y_train)\n",
    "    test_m = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'architecture': str(arch),\n",
    "        'n_layers': len(arch),\n",
    "        'total_neurons': sum(arch),\n",
    "        'train_rmse': train_m['rmse'],\n",
    "        'test_rmse': test_m['rmse'],\n",
    "        'iterations': model.model.n_iter_,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nArchitecture Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a445b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE by architecture\n",
    "x = range(len(results_df))\n",
    "axes[0].bar(x, results_df['train_rmse'], width=0.4, label='Train', color='steelblue', alpha=0.7)\n",
    "axes[0].bar([i+0.4 for i in x], results_df['test_rmse'], width=0.4, label='Test', color='coral', alpha=0.7)\n",
    "axes[0].set_xticks([i+0.2 for i in x])\n",
    "axes[0].set_xticklabels(results_df['architecture'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE by Architecture')\n",
    "axes[0].legend()\n",
    "\n",
    "# Overfitting (gap between train and test)\n",
    "overfit_gap = results_df['test_rmse'] - results_df['train_rmse']\n",
    "colors = ['red' if g > 0.1 else 'green' for g in overfit_gap]\n",
    "axes[1].bar(results_df['architecture'], overfit_gap, color=colors)\n",
    "axes[1].axhline(y=0.1, color='orange', linestyle='--', label='Warning threshold')\n",
    "axes[1].set_ylabel('Test RMSE - Train RMSE')\n",
    "axes[1].set_title('Overfitting Gap')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97205c",
   "metadata": {},
   "source": [
    "## NeuralNetworkGoalPredictor (Dual Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf30577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkGoalPredictor:\n",
    "    \"\"\"\n",
    "    Dual neural network for predicting both home and away goals.\n",
    "    Each model has its own scaler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params=None):\n",
    "        self.params = params\n",
    "        self.home_model = NeuralNetworkModel(params)\n",
    "        self.away_model = NeuralNetworkModel(params)\n",
    "        self.feature_columns = None\n",
    "    \n",
    "    def fit(self, df, feature_columns=None):\n",
    "        \"\"\"Fit both home and away models.\"\"\"\n",
    "        if feature_columns:\n",
    "            self.feature_columns = feature_columns\n",
    "        else:\n",
    "            exclude = ['home_goals', 'away_goals']\n",
    "            self.feature_columns = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                                    if c not in exclude]\n",
    "        \n",
    "        X = df[self.feature_columns]\n",
    "        \n",
    "        print(\"Training home goals model...\")\n",
    "        self.home_model.fit(X, df['home_goals'])\n",
    "        \n",
    "        print(\"\\nTraining away goals model...\")\n",
    "        self.away_model.fit(X, df['away_goals'])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_goals(self, df):\n",
    "        \"\"\"Predict home and away goals.\"\"\"\n",
    "        X = df[self.feature_columns]\n",
    "        return self.home_model.predict(X), self.away_model.predict(X)\n",
    "    \n",
    "    def predict_winner(self, df):\n",
    "        \"\"\"Predict game winners.\"\"\"\n",
    "        home_pred, away_pred = self.predict_goals(df)\n",
    "        results = []\n",
    "        for h, a in zip(home_pred, away_pred):\n",
    "            if h > a + 0.5:\n",
    "                results.append('home')\n",
    "            elif a > h + 0.5:\n",
    "                results.append('away')\n",
    "            else:\n",
    "                results.append('tie')\n",
    "        return pd.Series(results, index=df.index)\n",
    "    \n",
    "    def evaluate(self, df):\n",
    "        \"\"\"Evaluate both models.\"\"\"\n",
    "        home_pred, away_pred = self.predict_goals(df)\n",
    "        return {\n",
    "            'home_rmse': np.sqrt(mean_squared_error(df['home_goals'], home_pred)),\n",
    "            'away_rmse': np.sqrt(mean_squared_error(df['away_goals'], away_pred)),\n",
    "            'home_mae': mean_absolute_error(df['home_goals'], home_pred),\n",
    "            'away_mae': mean_absolute_error(df['away_goals'], away_pred),\n",
    "        }\n",
    "    \n",
    "    def get_loss_curves(self):\n",
    "        \"\"\"Get loss curves from both models.\"\"\"\n",
    "        return {\n",
    "            'home': self.home_model.get_loss_curve(),\n",
    "            'away': self.away_model.get_loss_curve(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0634e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dual predictor\n",
    "predictor = NeuralNetworkGoalPredictor({'hidden_layer_sizes': (128, 64)})\n",
    "predictor.fit(games_df, feature_cols)\n",
    "\n",
    "# Evaluate\n",
    "metrics = predictor.evaluate(games_df)\n",
    "print(\"\\nDual Model Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ddf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for both models\n",
    "loss_curves = predictor.get_loss_curves()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (name, curve) in enumerate(loss_curves.items()):\n",
    "    if curve:\n",
    "        axes[idx].plot(curve, color='steelblue', linewidth=2)\n",
    "        axes[idx].set_xlabel('Iteration')\n",
    "        axes[idx].set_ylabel('Loss')\n",
    "        axes[idx].set_title(f'{name.title()} Goals Model - Training Loss')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175261fe",
   "metadata": {},
   "source": [
    "## Hyperparameter Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha (L2 regularization)\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1]\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = NeuralNetworkModel({'alpha': alpha, 'max_iter': 200})\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_m = model.evaluate(X_train, y_train)\n",
    "    test_m = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'train_rmse': train_m['rmse'],\n",
    "        'test_rmse': test_m['rmse'],\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogx(results_df['alpha'], results_df['train_rmse'], 'o-', label='Train', color='steelblue')\n",
    "plt.semilogx(results_df['alpha'], results_df['test_rmse'], 's-', label='Test', color='coral')\n",
    "plt.xlabel('Alpha (L2 Regularization)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Effect of Regularization on Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRegularization Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f25d36",
   "metadata": {},
   "source": [
    "## Save/Load Model (with Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717de1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# CRITICAL: Must save both model AND scaler\n",
    "output_dir = Path('../output/models')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = output_dir / 'neural_network_model.pkl'\n",
    "\n",
    "# Save both model and scaler\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': nn_model.model,\n",
    "        'scaler': nn_model.scaler,  # CRITICAL!\n",
    "        'feature_names': nn_model.feature_names,\n",
    "        'params': nn_model.params,\n",
    "    }, f)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Load and verify\n",
    "with open(model_path, 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "\n",
    "# Use loaded scaler for prediction\n",
    "X_scaled = loaded['scaler'].transform(X_test.values)\n",
    "loaded_pred = loaded['model'].predict(X_scaled)\n",
    "print(f\"Loaded model RMSE: {np.sqrt(mean_squared_error(y_test, loaded_pred)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d73190",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Feature scaling is REQUIRED** for neural networks\n",
    "2. **Architecture matters**: Start simple, add complexity if needed\n",
    "3. **Early stopping** prevents overfitting\n",
    "4. **Alpha** controls L2 regularization strength\n",
    "5. **Save both model AND scaler** for deployment\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use StandardScaler for normally distributed features\n",
    "- Use RobustScaler if outliers are present\n",
    "- Start with (64, 32) architecture, adjust based on performance\n",
    "- Monitor loss curve to detect convergence issues\n",
    "- Use early_stopping=True to prevent overfitting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
