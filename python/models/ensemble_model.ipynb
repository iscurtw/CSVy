{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a349ff",
   "metadata": {},
   "source": [
    "# Ensemble Model - Implementation\n",
    "\n",
    "## Features\n",
    "\n",
    "- **EnsembleModel**: Combine multiple base models with weighted averaging\n",
    "- **StackedEnsemble**: Meta-learning with stacking regressor\n",
    "- **EnsembleGoalPredictor**: Dual ensemble for home/away goals\n",
    "- Weight optimization (equal, inverse-error, scipy-optimized)\n",
    "- Multiple combination methods (mean, median, min, max)\n",
    "\n",
    "## Ensemble Strategies\n",
    "\n",
    "| Strategy | Description | Best For |\n",
    "|----------|-------------|----------|\n",
    "| Simple Averaging | Equal weights | Diverse models |\n",
    "| Weighted Averaging | Custom weights | Known model quality |\n",
    "| Inverse-Error | Weight by validation error | Auto-tuning |\n",
    "| Stacking | Meta-learner combines | Maximum performance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d35e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Ensemble methods ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865bb0e",
   "metadata": {},
   "source": [
    "## Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f737a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hockey_data(n_games=1000):\n",
    "    \"\"\"Generate synthetic hockey data with realistic features.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = {\n",
    "        'home_elo': np.random.normal(1500, 100, n_games),\n",
    "        'away_elo': np.random.normal(1500, 100, n_games),\n",
    "        'home_goals_avg': np.random.uniform(2.5, 3.5, n_games),\n",
    "        'away_goals_avg': np.random.uniform(2.5, 3.5, n_games),\n",
    "        'home_goals_against_avg': np.random.uniform(2.5, 3.5, n_games),\n",
    "        'away_goals_against_avg': np.random.uniform(2.5, 3.5, n_games),\n",
    "        'home_pp_pct': np.random.uniform(0.15, 0.30, n_games),\n",
    "        'away_pp_pct': np.random.uniform(0.15, 0.30, n_games),\n",
    "        'home_pk_pct': np.random.uniform(0.75, 0.90, n_games),\n",
    "        'away_pk_pct': np.random.uniform(0.75, 0.90, n_games),\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['elo_diff'] = df['home_elo'] - df['away_elo']\n",
    "    \n",
    "    # Generate realistic goals\n",
    "    home_base = 3.0 + 0.001 * df['elo_diff'] + 0.3 * (df['home_goals_avg'] - 3.0)\n",
    "    away_base = 3.0 - 0.001 * df['elo_diff'] + 0.3 * (df['away_goals_avg'] - 3.0)\n",
    "    \n",
    "    df['home_goals'] = np.random.poisson(np.maximum(home_base, 1.5))\n",
    "    df['away_goals'] = np.random.poisson(np.maximum(away_base, 1.5))\n",
    "    \n",
    "    return df\n",
    "\n",
    "games_df = generate_hockey_data(1000)\n",
    "print(f\"Generated {len(games_df)} games\")\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = [c for c in games_df.columns if c not in ['home_goals', 'away_goals']]\n",
    "X = games_df[feature_cols]\n",
    "y = games_df['home_goals']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6407660",
   "metadata": {},
   "source": [
    "## Train Individual Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aac713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models\n",
    "base_models = {\n",
    "    'ridge': Ridge(alpha=1.0),\n",
    "    'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    'rf': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'gbm': GradientBoostingRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42),\n",
    "}\n",
    "\n",
    "# Train each model\n",
    "fitted_models = {}\n",
    "model_predictions = {}\n",
    "model_val_rmse = {}\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    fitted = clone(model).fit(X_train, y_train)\n",
    "    fitted_models[name] = fitted\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred = fitted.predict(X_val)\n",
    "    test_pred = fitted.predict(X_test)\n",
    "    model_predictions[name] = test_pred\n",
    "    \n",
    "    # Validation RMSE\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    model_val_rmse[name] = val_rmse\n",
    "    \n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    print(f\"{name:10s} - Val RMSE: {val_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceda3fe",
   "metadata": {},
   "source": [
    "## Simple Averaging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnsemble:\n",
    "    \"\"\"Simple averaging ensemble.\"\"\"\n",
    "    \n",
    "    def __init__(self, models):\n",
    "        self.models = models  # dict of name: fitted_model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Average predictions from all models.\"\"\"\n",
    "        predictions = np.column_stack([m.predict(X) for m in self.models.values()])\n",
    "        return predictions.mean(axis=1)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y, pred)),\n",
    "            'mae': mean_absolute_error(y, pred),\n",
    "            'r2': r2_score(y, pred),\n",
    "        }\n",
    "\n",
    "# Create simple ensemble\n",
    "simple_ensemble = SimpleEnsemble(fitted_models)\n",
    "simple_metrics = simple_ensemble.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nSimple Averaging Ensemble:\")\n",
    "for k, v in simple_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7631c",
   "metadata": {},
   "source": [
    "## Weighted Averaging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEnsemble:\n",
    "    \"\"\"Weighted averaging ensemble.\"\"\"\n",
    "    \n",
    "    def __init__(self, models, weights=None):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        if weights is None:\n",
    "            # Equal weights\n",
    "            n = len(models)\n",
    "            self.weights = {name: 1/n for name in models.keys()}\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Weighted average of predictions.\"\"\"\n",
    "        predictions = np.zeros(len(X))\n",
    "        for name, model in self.models.items():\n",
    "            predictions += self.weights[name] * model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y, pred)),\n",
    "            'mae': mean_absolute_error(y, pred),\n",
    "            'r2': r2_score(y, pred),\n",
    "        }\n",
    "\n",
    "# Compute inverse-error weights\n",
    "inverse_errors = {name: 1/rmse for name, rmse in model_val_rmse.items()}\n",
    "total = sum(inverse_errors.values())\n",
    "weights = {name: v/total for name, v in inverse_errors.items()}\n",
    "\n",
    "print(\"Inverse-Error Weights:\")\n",
    "for name, w in weights.items():\n",
    "    print(f\"  {name}: {w:.4f}\")\n",
    "\n",
    "# Create weighted ensemble\n",
    "weighted_ensemble = WeightedEnsemble(fitted_models, weights)\n",
    "weighted_metrics = weighted_ensemble.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nWeighted Ensemble Metrics:\")\n",
    "for k, v in weighted_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9c9aa",
   "metadata": {},
   "source": [
    "## Optimized Weights with Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f31624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Get validation predictions from all models\n",
    "val_preds = np.column_stack([m.predict(X_val) for m in fitted_models.values()])\n",
    "\n",
    "def objective(weights):\n",
    "    \"\"\"MSE of weighted combination.\"\"\"\n",
    "    combined = val_preds @ weights\n",
    "    return mean_squared_error(y_val, combined)\n",
    "\n",
    "# Constraints: weights sum to 1, all >= 0\n",
    "n_models = len(fitted_models)\n",
    "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "bounds = [(0, 1) for _ in range(n_models)]\n",
    "initial = np.ones(n_models) / n_models\n",
    "\n",
    "# Optimize\n",
    "result = minimize(objective, initial, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "optimized_weights = dict(zip(fitted_models.keys(), result.x))\n",
    "\n",
    "print(\"Optimized Weights:\")\n",
    "for name, w in optimized_weights.items():\n",
    "    print(f\"  {name}: {w:.4f}\")\n",
    "\n",
    "# Evaluate optimized ensemble\n",
    "opt_ensemble = WeightedEnsemble(fitted_models, optimized_weights)\n",
    "opt_metrics = opt_ensemble.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nOptimized Ensemble Metrics:\")\n",
    "for k, v in opt_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c543a",
   "metadata": {},
   "source": [
    "## Stacking Ensemble (Meta-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f873a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacking regressor\n",
    "estimators = [\n",
    "    ('ridge', Ridge(alpha=1.0)),\n",
    "    ('elastic', ElasticNet(alpha=0.1, l1_ratio=0.5)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "    ('gbm', GradientBoostingRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42)),\n",
    "]\n",
    "\n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=Ridge(alpha=0.1),\n",
    "    cv=5,\n",
    "    passthrough=False,  # Only use base model predictions\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on train+val (stacking handles CV internally)\n",
    "X_train_full = pd.concat([X_train, X_val])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "print(\"Fitting stacking ensemble (this may take a moment)...\")\n",
    "stacking.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Evaluate\n",
    "stack_pred = stacking.predict(X_test)\n",
    "stack_rmse = np.sqrt(mean_squared_error(y_test, stack_pred))\n",
    "stack_mae = mean_absolute_error(y_test, stack_pred)\n",
    "stack_r2 = r2_score(y_test, stack_pred)\n",
    "\n",
    "print(f\"\\nStacking Ensemble Metrics:\")\n",
    "print(f\"  RMSE: {stack_rmse:.4f}\")\n",
    "print(f\"  MAE:  {stack_mae:.4f}\")\n",
    "print(f\"  R²:   {stack_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e16a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect meta-learner weights\n",
    "meta_model = stacking.final_estimator_\n",
    "if hasattr(meta_model, 'coef_'):\n",
    "    meta_weights = pd.Series(meta_model.coef_, index=[n for n, _ in estimators])\n",
    "    print(\"Meta-Learner (Ridge) Weights:\")\n",
    "    print(meta_weights)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    meta_weights.plot(kind='bar', color='steelblue')\n",
    "    plt.title('Stacking Meta-Learner Weights')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba9716",
   "metadata": {},
   "source": [
    "## Compare All Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746da0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "comparison = {\n",
    "    'Ridge (single)': np.sqrt(mean_squared_error(y_test, fitted_models['ridge'].predict(X_test))),\n",
    "    'RF (single)': np.sqrt(mean_squared_error(y_test, fitted_models['rf'].predict(X_test))),\n",
    "    'GBM (single)': np.sqrt(mean_squared_error(y_test, fitted_models['gbm'].predict(X_test))),\n",
    "    'Simple Average': simple_metrics['rmse'],\n",
    "    'Inverse-Error Weighted': weighted_metrics['rmse'],\n",
    "    'Optimized Weights': opt_metrics['rmse'],\n",
    "    'Stacking': stack_rmse,\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "methods = list(comparison.keys())\n",
    "rmses = list(comparison.values())\n",
    "colors = ['lightgray'] * 3 + ['steelblue'] * 3 + ['forestgreen']\n",
    "\n",
    "bars = plt.barh(methods, rmses, color=colors)\n",
    "plt.xlabel('Test RMSE (lower is better)')\n",
    "plt.title('Ensemble Method Comparison')\n",
    "\n",
    "# Add value labels\n",
    "for bar, rmse in zip(bars, rmses):\n",
    "    plt.text(rmse + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{rmse:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "for method, rmse in sorted(comparison.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {method:25s}: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716a4e4",
   "metadata": {},
   "source": [
    "## EnsembleGoalPredictor (Dual Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb74c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleGoalPredictor:\n",
    "    \"\"\"Dual ensemble for home and away goal prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_models, weights='inverse_error'):\n",
    "        self.base_models = base_models  # List of (name, model) tuples\n",
    "        self.weights = weights\n",
    "        self.home_models = {}\n",
    "        self.away_models = {}\n",
    "        self.home_weights = None\n",
    "        self.away_weights = None\n",
    "        self.feature_columns = None\n",
    "    \n",
    "    def fit(self, df, feature_columns=None, val_fraction=0.2):\n",
    "        \"\"\"Fit ensembles for home and away goals.\"\"\"\n",
    "        # Get features\n",
    "        if feature_columns:\n",
    "            self.feature_columns = feature_columns\n",
    "        else:\n",
    "            exclude = ['home_goals', 'away_goals']\n",
    "            self.feature_columns = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                                    if c not in exclude]\n",
    "        \n",
    "        X = df[self.feature_columns]\n",
    "        y_home = df['home_goals']\n",
    "        y_away = df['away_goals']\n",
    "        \n",
    "        # Split for weight optimization\n",
    "        n_val = int(len(df) * val_fraction)\n",
    "        X_train, X_val = X.iloc[:-n_val], X.iloc[-n_val:]\n",
    "        y_home_train, y_home_val = y_home.iloc[:-n_val], y_home.iloc[-n_val:]\n",
    "        y_away_train, y_away_val = y_away.iloc[:-n_val], y_away.iloc[-n_val:]\n",
    "        \n",
    "        # Fit home models\n",
    "        home_errors = {}\n",
    "        for name, model in self.base_models:\n",
    "            fitted = clone(model).fit(X_train, y_home_train)\n",
    "            self.home_models[name] = fitted\n",
    "            val_pred = fitted.predict(X_val)\n",
    "            home_errors[name] = np.sqrt(mean_squared_error(y_home_val, val_pred))\n",
    "        \n",
    "        # Fit away models\n",
    "        away_errors = {}\n",
    "        for name, model in self.base_models:\n",
    "            fitted = clone(model).fit(X_train, y_away_train)\n",
    "            self.away_models[name] = fitted\n",
    "            val_pred = fitted.predict(X_val)\n",
    "            away_errors[name] = np.sqrt(mean_squared_error(y_away_val, val_pred))\n",
    "        \n",
    "        # Compute weights\n",
    "        if self.weights == 'inverse_error':\n",
    "            home_inv = {n: 1/e for n, e in home_errors.items()}\n",
    "            away_inv = {n: 1/e for n, e in away_errors.items()}\n",
    "            home_total = sum(home_inv.values())\n",
    "            away_total = sum(away_inv.values())\n",
    "            self.home_weights = {n: v/home_total for n, v in home_inv.items()}\n",
    "            self.away_weights = {n: v/away_total for n, v in away_inv.items()}\n",
    "        else:\n",
    "            n = len(self.base_models)\n",
    "            self.home_weights = {n: 1/n for n, _ in self.base_models}\n",
    "            self.away_weights = {n: 1/n for n, _ in self.base_models}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_goals(self, df):\n",
    "        \"\"\"Predict home and away goals.\"\"\"\n",
    "        X = df[self.feature_columns]\n",
    "        \n",
    "        home_pred = np.zeros(len(X))\n",
    "        for name, model in self.home_models.items():\n",
    "            home_pred += self.home_weights[name] * model.predict(X)\n",
    "        \n",
    "        away_pred = np.zeros(len(X))\n",
    "        for name, model in self.away_models.items():\n",
    "            away_pred += self.away_weights[name] * model.predict(X)\n",
    "        \n",
    "        return home_pred, away_pred\n",
    "    \n",
    "    def evaluate(self, df):\n",
    "        \"\"\"Evaluate prediction performance.\"\"\"\n",
    "        home_pred, away_pred = self.predict_goals(df)\n",
    "        return {\n",
    "            'home_rmse': np.sqrt(mean_squared_error(df['home_goals'], home_pred)),\n",
    "            'away_rmse': np.sqrt(mean_squared_error(df['away_goals'], away_pred)),\n",
    "            'home_mae': mean_absolute_error(df['home_goals'], home_pred),\n",
    "            'away_mae': mean_absolute_error(df['away_goals'], away_pred),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50112d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dual ensemble predictor\n",
    "base_models = [\n",
    "    ('ridge', Ridge(alpha=1.0)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)),\n",
    "    ('gbm', GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)),\n",
    "]\n",
    "\n",
    "predictor = EnsembleGoalPredictor(base_models)\n",
    "predictor.fit(games_df, feature_cols)\n",
    "\n",
    "print(\"\\nHome Model Weights:\")\n",
    "for name, w in predictor.home_weights.items():\n",
    "    print(f\"  {name}: {w:.4f}\")\n",
    "\n",
    "print(\"\\nAway Model Weights:\")\n",
    "for name, w in predictor.away_weights.items():\n",
    "    print(f\"  {name}: {w:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "metrics = predictor.evaluate(games_df)\n",
    "print(\"\\nDual Ensemble Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83552df5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Simple Averaging** works well when models have similar accuracy\n",
    "2. **Inverse-Error Weighting** automatically favors better models\n",
    "3. **Optimized Weights** can find the best linear combination\n",
    "4. **Stacking** learns non-linear combinations via meta-model\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use diverse models (linear + tree-based) for best ensembles\n",
    "- Always use validation set for weight optimization (not test set!)\n",
    "- Stacking with CV prevents overfitting to training data\n",
    "- More models ≠ better ensemble (diminishing returns)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
