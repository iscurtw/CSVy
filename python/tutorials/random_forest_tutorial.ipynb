{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e941c8",
   "metadata": {},
   "source": [
    "# Random Forest Tutorial for Hockey Prediction\n",
    "\n",
    "This tutorial explains how Random Forest works and how to use it for\n",
    "predicting hockey game outcomes.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Random Forest Basics** - How ensemble of trees works\n",
    "2. **Bagging vs Boosting** - Key differences from XGBoost\n",
    "3. **Key Hyperparameters** - Trees, depth, samples\n",
    "4. **Out-of-Bag Error** - Free validation!\n",
    "5. **Feature Importance** - Understanding what matters\n",
    "6. **Practical Usage** - Training and prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea301483",
   "metadata": {},
   "source": [
    "## 1. Understanding Random Forest\n",
    "\n",
    "Random Forest builds **many independent trees** and averages their predictions.\n",
    "\n",
    "$$\\hat{y} = \\frac{1}{K} \\sum_{k=1}^{K} f_k(x)$$\n",
    "\n",
    "Where:\n",
    "- $K$ = number of trees (n_estimators)\n",
    "- $f_k$ = individual tree prediction\n",
    "\n",
    "### Two Sources of Randomness\n",
    "\n",
    "1. **Bootstrap sampling**: Each tree sees a random ~63% of data\n",
    "2. **Feature sampling**: Each split considers random subset of features\n",
    "\n",
    "This creates **diverse trees** that make different errors, which cancel out!\n",
    "\n",
    "### Random Forest vs XGBoost\n",
    "\n",
    "| Aspect | Random Forest | XGBoost |\n",
    "|--------|--------------|----------|\n",
    "| Training | Parallel (fast) | Sequential |\n",
    "| Trees | Independent | Each corrects previous |\n",
    "| Overfitting | Less prone | More prone |\n",
    "| Tuning | Easier | More parameters |\n",
    "| Speed | Faster training | Often faster inference |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b097d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print(\"Tutorial ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971eb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample hockey data\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'elo_diff': np.random.normal(0, 100, n),\n",
    "    'home_win_pct': np.random.uniform(0.35, 0.65, n),\n",
    "    'away_win_pct': np.random.uniform(0.35, 0.65, n),\n",
    "    'home_goals_avg': np.random.uniform(2.5, 3.5, n),\n",
    "    'away_goals_against': np.random.uniform(2.5, 3.5, n),\n",
    "    'home_pp_pct': np.random.uniform(0.15, 0.25, n),\n",
    "    'away_pk_pct': np.random.uniform(0.75, 0.88, n),\n",
    "    'rest_days': np.random.choice([1, 2, 3, 4], n),\n",
    "})\n",
    "\n",
    "# Target with non-linear relationships\n",
    "data['home_goals'] = (\n",
    "    2.5 +\n",
    "    0.005 * data['elo_diff'] +\n",
    "    0.8 * data['home_win_pct'] +\n",
    "    0.4 * data['home_goals_avg'] +\n",
    "    4 * data['home_pp_pct'] * (1 - data['away_pk_pct']) +  # Interaction!\n",
    "    np.where(data['rest_days'] >= 3, 0.3, 0) +\n",
    "    np.random.normal(0, 0.5, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Goals: {data['home_goals'].min()} - {data['home_goals'].max()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f49c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "feature_cols = [c for c in data.columns if c != 'home_goals']\n",
    "X = data[feature_cols]\n",
    "y = data['home_goals']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Training: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aece6c",
   "metadata": {},
   "source": [
    "## 2. Key Hyperparameters\n",
    "\n",
    "Random Forest has fewer critical parameters than XGBoost:\n",
    "\n",
    "| Parameter | Range | Effect |\n",
    "|-----------|-------|--------|\n",
    "| `n_estimators` | 100-500 | More trees = better (diminishing returns) |\n",
    "| `max_depth` | None, 10-30 | None = fully grown trees |\n",
    "| `min_samples_split` | 2-10 | Minimum samples to split a node |\n",
    "| `min_samples_leaf` | 1-5 | Minimum samples in leaf nodes |\n",
    "| `max_features` | 'sqrt', 0.5-1.0 | Features considered per split |\n",
    "| `bootstrap` | True | Use bootstrap sampling |\n",
    "| `oob_score` | True | Compute out-of-bag error |\n",
    "\n",
    "### Golden Rules\n",
    "- **More trees is almost always better** (no overfitting risk)\n",
    "- **Deeper trees = more complex model** (but still robust)\n",
    "- **max_features='sqrt' is a great default**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare number of trees\n",
    "tree_counts = [10, 50, 100, 200, 500]\n",
    "results = []\n",
    "\n",
    "for n_trees in tree_counts:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_trees,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all CPU cores\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, model.predict(X_train)))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test)))\n",
    "    \n",
    "    results.append({\n",
    "        'n_estimators': n_trees,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Number of Trees Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nNote: Test error stabilizes after ~100 trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trees vs error\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(results_df['n_estimators'], results_df['train_rmse'], \n",
    "        'o-', label='Train RMSE', color='blue')\n",
    "ax.plot(results_df['n_estimators'], results_df['test_rmse'], \n",
    "        's-', label='Test RMSE', color='orange')\n",
    "\n",
    "ax.set_xlabel('Number of Trees')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Random Forest: More Trees → Diminishing Returns')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c47fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare max_depth\n",
    "depths = [5, 10, 15, 20, None]  # None = unlimited\n",
    "depth_results = []\n",
    "\n",
    "for depth in depths:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=depth,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, model.predict(X_train)))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test)))\n",
    "    \n",
    "    depth_results.append({\n",
    "        'max_depth': str(depth) if depth else 'None',\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "    })\n",
    "\n",
    "depth_df = pd.DataFrame(depth_results)\n",
    "print(\"Max Depth Comparison:\")\n",
    "print(depth_df.to_string(index=False))\n",
    "print(\"\\nNote: RF is robust - even None (fully grown) often works well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c302bc5",
   "metadata": {},
   "source": [
    "## 3. Out-of-Bag (OOB) Error\n",
    "\n",
    "A unique feature of Random Forest: **free validation**!\n",
    "\n",
    "### How It Works\n",
    "\n",
    "- Each tree is trained on ~63% of data (bootstrap sample)\n",
    "- The other ~37% is \"out-of-bag\" for that tree\n",
    "- Each sample is OOB for about 1/3 of trees\n",
    "- Predict using only trees where that sample was OOB\n",
    "\n",
    "**OOB error ≈ Cross-validation error** (but faster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78033aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable OOB scoring\n",
    "model_oob = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    oob_score=True,  # Enable OOB\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_oob.fit(X_train, y_train)\n",
    "\n",
    "# OOB R² score\n",
    "oob_r2 = model_oob.oob_score_\n",
    "\n",
    "# Compare to actual test performance\n",
    "test_pred = model_oob.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "print(f\"OOB R² Score: {oob_r2:.4f}\")\n",
    "print(f\"Test R² Score: {test_r2:.4f}\")\n",
    "print(f\"\\nDifference: {abs(oob_r2 - test_r2):.4f}\")\n",
    "print(\"OOB closely approximates test performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d82a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB predictions for each training sample\n",
    "oob_predictions = model_oob.oob_prediction_\n",
    "oob_rmse = np.sqrt(mean_squared_error(y_train, oob_predictions))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "print(f\"OOB RMSE: {oob_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(\"\\n→ Use OOB to estimate performance without held-out set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a8f67f",
   "metadata": {},
   "source": [
    "## 4. Feature Importance\n",
    "\n",
    "Random Forest provides **impurity-based** importance:\n",
    "\n",
    "- How much does each feature reduce variance (for regression)?\n",
    "- Averaged across all trees and all splits\n",
    "\n",
    "### Caution\n",
    "Impurity importance can be biased toward:\n",
    "- High cardinality features\n",
    "- Correlated features\n",
    "\n",
    "For better importance: use **permutation importance** (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198432a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (impurity-based):\")\n",
    "print(importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dbc74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance (more reliable)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    model, X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "perm_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': perm_importance.importances_mean,\n",
    "    'std': perm_importance.importances_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Permutation Importance (more reliable):\")\n",
    "print(perm_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both types of importance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Impurity-based\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance)))\n",
    "ax1.barh(importance['feature'], importance['importance'], color=colors)\n",
    "ax1.set_xlabel('Importance')\n",
    "ax1.set_title('Impurity-Based Importance')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Permutation\n",
    "ax2.barh(perm_df['feature'], perm_df['importance'], \n",
    "         xerr=perm_df['std'], color=colors, capsize=3)\n",
    "ax2.set_xlabel('Importance (decrease in R²)')\n",
    "ax2.set_title('Permutation Importance')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e55787",
   "metadata": {},
   "source": [
    "## 5. Practical Usage: Goal Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model with recommended settings\n",
    "final_model = RandomForestRegressor(\n",
    "    n_estimators=200,       # Enough trees\n",
    "    max_depth=15,           # Limit depth slightly\n",
    "    min_samples_split=5,    # Don't split tiny nodes\n",
    "    min_samples_leaf=2,     # Require 2+ samples per leaf\n",
    "    max_features='sqrt',    # Classic RF default\n",
    "    oob_score=True,         # Free validation\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_pred = final_model.predict(X_train)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "print(\"Final Model Performance:\")\n",
    "print(f\"  OOB R²:    {final_model.oob_score_:.4f}\")\n",
    "print(f\"  Test RMSE: {np.sqrt(mean_squared_error(y_test, test_pred)):.4f}\")\n",
    "print(f\"  Test MAE:  {mean_absolute_error(y_test, test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    final_model, X, y, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"5-Fold Cross-Validation:\")\n",
    "print(f\"  RMSE: {-cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for a new game\n",
    "new_game = pd.DataFrame([{\n",
    "    'elo_diff': 100,\n",
    "    'home_win_pct': 0.55,\n",
    "    'away_win_pct': 0.45,\n",
    "    'home_goals_avg': 3.2,\n",
    "    'away_goals_against': 3.0,\n",
    "    'home_pp_pct': 0.22,\n",
    "    'away_pk_pct': 0.82,\n",
    "    'rest_days': 2,\n",
    "}])\n",
    "\n",
    "predicted_goals = final_model.predict(new_game)[0]\n",
    "print(f\"Predicted home goals: {predicted_goals:.2f}\")\n",
    "print(f\"Rounded: {round(predicted_goals)} goals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d03db",
   "metadata": {},
   "source": [
    "## 6. Random Forest vs XGBoost: When to Use Which?\n",
    "\n",
    "### Use Random Forest When:\n",
    "- You want **simplicity** (fewer hyperparameters)\n",
    "- You need **interpretable feature importance**\n",
    "- You have **limited tuning time**\n",
    "- Your data has **outliers** (RF is more robust)\n",
    "- You want **parallel training** (faster on multi-core)\n",
    "\n",
    "### Use XGBoost When:\n",
    "- You want **maximum accuracy** (usually wins competitions)\n",
    "- You have **time to tune** hyperparameters\n",
    "- You need **early stopping** (built-in)\n",
    "- Your data is **clean and structured**\n",
    "\n",
    "### For Hockey Predictions:\n",
    "Both work well! Random Forest is a great starting point, then try XGBoost if you need more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of recommended parameters\n",
    "print(\"=\"*50)\n",
    "print(\" RECOMMENDED RANDOM FOREST PARAMETERS FOR HOCKEY\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "params = {\n",
    "    'n_estimators': 200,      # 100-500, more is better\n",
    "    'max_depth': 15,          # 10-20, or None\n",
    "    'min_samples_split': 5,   # 2-10\n",
    "    'min_samples_leaf': 2,    # 1-4\n",
    "    'max_features': 'sqrt',   # Classic default\n",
    "    'oob_score': True,        # Free validation!\n",
    "    'n_jobs': -1,             # Use all CPU cores\n",
    "}\n",
    "\"\"\")\n",
    "print(\"Tutorial complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
