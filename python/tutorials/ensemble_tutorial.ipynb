{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5684e6",
   "metadata": {},
   "source": [
    "# Ensemble Methods Tutorial for Hockey Prediction\n",
    "\n",
    "This tutorial explains how to combine multiple models for better predictions.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Why Ensembles Work** - Wisdom of crowds\n",
    "2. **Simple Averaging** - Combine equal-weight predictions\n",
    "3. **Weighted Averaging** - Give better models more weight\n",
    "4. **Stacking** - Use a meta-model to combine predictions\n",
    "5. **Blending** - Practical ensemble strategy\n",
    "6. **Practical Usage** - Building your ensemble\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136eb472",
   "metadata": {},
   "source": [
    "## 1. Why Ensembles Work\n",
    "\n",
    "Different models make **different errors**. By combining them,\n",
    "errors can cancel out!\n",
    "\n",
    "### The Math\n",
    "\n",
    "If we have 3 uncorrelated models with equal variance $\\sigma^2$:\n",
    "\n",
    "$$\\text{Variance of average} = \\frac{\\sigma^2}{3}$$\n",
    "\n",
    "**Error reduces as we add diverse models!**\n",
    "\n",
    "### Key Insight\n",
    "Ensembles work best when models are:\n",
    "- **Different** (linear + tree + neural network)\n",
    "- **Accurate** (each model should be decent on its own)\n",
    "- **Uncorrelated** in their errors\n",
    "\n",
    "### Common Ensemble Types\n",
    "\n",
    "| Method | Description | When to Use |\n",
    "|--------|-------------|-------------|\n",
    "| Averaging | Mean of predictions | Quick improvement |\n",
    "| Weighted | Weighted mean | Models have different skill |\n",
    "| Stacking | Meta-model learns weights | Maximum accuracy |\n",
    "| Blending | Like stacking, simpler | Practical compromise |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd948b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check for XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not available, using GradientBoosting instead\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Tutorial ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03484b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample hockey data\n",
    "np.random.seed(42)\n",
    "n = 600\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'elo_diff': np.random.normal(0, 100, n),\n",
    "    'home_win_pct': np.random.uniform(0.35, 0.65, n),\n",
    "    'away_win_pct': np.random.uniform(0.35, 0.65, n),\n",
    "    'home_goals_avg': np.random.uniform(2.5, 3.5, n),\n",
    "    'away_goals_against': np.random.uniform(2.5, 3.5, n),\n",
    "    'home_pp_pct': np.random.uniform(0.15, 0.25, n),\n",
    "    'rest_days': np.random.choice([1, 2, 3, 4], n),\n",
    "})\n",
    "\n",
    "# Target with both linear and non-linear components\n",
    "# (so different models capture different aspects)\n",
    "data['home_goals'] = (\n",
    "    2.5 +\n",
    "    # Linear component (Ridge will capture this)\n",
    "    0.005 * data['elo_diff'] +\n",
    "    0.6 * data['home_win_pct'] +\n",
    "    # Non-linear component (Trees will capture this)\n",
    "    np.where(data['elo_diff'] > 50, 0.4, 0) +\n",
    "    np.where(data['rest_days'] >= 3, 0.3, 0) +\n",
    "    # Interaction (complex models will capture this)\n",
    "    0.3 * data['home_goals_avg'] * data['home_pp_pct'] * 4 +\n",
    "    np.random.normal(0, 0.6, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5493f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: train / validation / test\n",
    "feature_cols = [c for c in data.columns if c != 'home_goals']\n",
    "X = data[feature_cols]\n",
    "y = data['home_goals']\n",
    "\n",
    "# 60% train, 20% validation (for ensemble weights), 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5116f37",
   "metadata": {},
   "source": [
    "## 2. Training Base Models\n",
    "\n",
    "Let's train diverse models that will form our ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadfaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data for linear models and neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define base models\n",
    "models = {\n",
    "    'ridge': Ridge(alpha=1.0),\n",
    "    'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    'rf': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'mlp': MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42),\n",
    "}\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    models['xgb'] = xgb.XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, \n",
    "                                     random_state=42, verbosity=0)\n",
    "else:\n",
    "    models['gb'] = GradientBoostingRegressor(n_estimators=100, max_depth=4, \n",
    "                                             learning_rate=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training {len(models)} base models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be2b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and get predictions\n",
    "val_predictions = {}\n",
    "test_predictions = {}\n",
    "model_scores = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for linear/MLP, raw for trees\n",
    "    if name in ['ridge', 'elastic', 'mlp']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        val_pred = model.predict(X_val_scaled)\n",
    "        test_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "        test_pred = model.predict(X_test)\n",
    "    \n",
    "    val_predictions[name] = val_pred\n",
    "    test_predictions[name] = test_pred\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    model_scores.append({'model': name, 'val_rmse': rmse})\n",
    "    print(f\"  {name}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "scores_df = pd.DataFrame(model_scores).sort_values('val_rmse')\n",
    "print(\"\\nModel Rankings:\")\n",
    "print(scores_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ba299",
   "metadata": {},
   "source": [
    "## 3. Simple Averaging\n",
    "\n",
    "The simplest ensemble: just average all predictions!\n",
    "\n",
    "$$\\hat{y}_{ensemble} = \\frac{1}{K} \\sum_{k=1}^{K} \\hat{y}_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d06d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple average of all models\n",
    "val_avg = np.mean(list(val_predictions.values()), axis=0)\n",
    "test_avg = np.mean(list(test_predictions.values()), axis=0)\n",
    "\n",
    "avg_rmse_val = np.sqrt(mean_squared_error(y_val, val_avg))\n",
    "avg_rmse_test = np.sqrt(mean_squared_error(y_test, test_avg))\n",
    "\n",
    "print(f\"Simple Average Ensemble:\")\n",
    "print(f\"  Validation RMSE: {avg_rmse_val:.4f}\")\n",
    "print(f\"  Test RMSE:       {avg_rmse_test:.4f}\")\n",
    "\n",
    "# Compare to best single model\n",
    "best_single = scores_df.iloc[0]\n",
    "improvement = (best_single['val_rmse'] - avg_rmse_val) / best_single['val_rmse'] * 100\n",
    "print(f\"\\nBest single model ({best_single['model']}): {best_single['val_rmse']:.4f}\")\n",
    "print(f\"Improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32586f",
   "metadata": {},
   "source": [
    "## 4. Weighted Averaging\n",
    "\n",
    "Give better models more influence:\n",
    "\n",
    "$$\\hat{y}_{ensemble} = \\sum_{k=1}^{K} w_k \\hat{y}_k, \\quad \\sum w_k = 1$$\n",
    "\n",
    "### Methods to find weights:\n",
    "1. **Inverse error**: $w_k \\propto \\frac{1}{RMSE_k}$\n",
    "2. **Rank-based**: Better models get higher rank weights\n",
    "3. **Optimization**: Find weights that minimize validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7634057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Inverse RMSE weights\n",
    "inverse_weights = {}\n",
    "for name, pred in val_predictions.items():\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    inverse_weights[name] = 1 / rmse\n",
    "\n",
    "# Normalize to sum to 1\n",
    "total = sum(inverse_weights.values())\n",
    "inverse_weights = {k: v/total for k, v in inverse_weights.items()}\n",
    "\n",
    "print(\"Inverse RMSE Weights:\")\n",
    "for name, weight in sorted(inverse_weights.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply weighted average\n",
    "val_weighted = np.zeros(len(y_val))\n",
    "test_weighted = np.zeros(len(y_test))\n",
    "\n",
    "for name, weight in inverse_weights.items():\n",
    "    val_weighted += weight * val_predictions[name]\n",
    "    test_weighted += weight * test_predictions[name]\n",
    "\n",
    "weighted_rmse_val = np.sqrt(mean_squared_error(y_val, val_weighted))\n",
    "weighted_rmse_test = np.sqrt(mean_squared_error(y_test, test_weighted))\n",
    "\n",
    "print(f\"Weighted Average Ensemble:\")\n",
    "print(f\"  Validation RMSE: {weighted_rmse_val:.4f}\")\n",
    "print(f\"  Test RMSE:       {weighted_rmse_test:.4f}\")\n",
    "print(f\"\\nCompared to simple average: {avg_rmse_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Optimized weights using scipy\n",
    "try:\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    # Stack predictions into matrix\n",
    "    val_matrix = np.column_stack(list(val_predictions.values()))\n",
    "    test_matrix = np.column_stack(list(test_predictions.values()))\n",
    "    \n",
    "    def objective(weights):\n",
    "        pred = val_matrix @ weights\n",
    "        return mean_squared_error(y_val, pred)\n",
    "    \n",
    "    # Constraints: weights sum to 1, all non-negative\n",
    "    n_models = len(models)\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    bounds = [(0, 1) for _ in range(n_models)]\n",
    "    \n",
    "    # Initial guess: equal weights\n",
    "    initial = np.ones(n_models) / n_models\n",
    "    \n",
    "    result = minimize(objective, initial, method='SLSQP', \n",
    "                      bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    optimal_weights = result.x\n",
    "    model_names = list(val_predictions.keys())\n",
    "    \n",
    "    print(\"Optimized Weights:\")\n",
    "    for name, weight in zip(model_names, optimal_weights):\n",
    "        print(f\"  {name}: {weight:.3f}\")\n",
    "    \n",
    "    # Apply optimal weights\n",
    "    val_optimal = val_matrix @ optimal_weights\n",
    "    test_optimal = test_matrix @ optimal_weights\n",
    "    \n",
    "    optimal_rmse_val = np.sqrt(mean_squared_error(y_val, val_optimal))\n",
    "    optimal_rmse_test = np.sqrt(mean_squared_error(y_test, test_optimal))\n",
    "    \n",
    "    print(f\"\\nOptimal Ensemble:\")\n",
    "    print(f\"  Validation RMSE: {optimal_rmse_val:.4f}\")\n",
    "    print(f\"  Test RMSE:       {optimal_rmse_test:.4f}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"scipy not available for optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630c3a0",
   "metadata": {},
   "source": [
    "## 5. Stacking (Meta-Learning)\n",
    "\n",
    "Use a **meta-model** to learn how to combine base model predictions.\n",
    "\n",
    "### How It Works:\n",
    "1. Train base models on training data\n",
    "2. Get predictions from base models (using cross-validation!)\n",
    "3. Train a meta-model using predictions as features\n",
    "4. Final prediction = meta-model(base predictions)\n",
    "\n",
    "### Why Cross-Validation?\n",
    "If we use training predictions, base models have \"seen\" the data.\n",
    "Their predictions are too good → meta-model overfits.\n",
    "\n",
    "Using **out-of-fold predictions** gives honest estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87666a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking with sklearn\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Define base estimators\n",
    "base_estimators = [\n",
    "    ('ridge', Ridge(alpha=1.0)),\n",
    "    ('elastic', ElasticNet(alpha=0.1, l1_ratio=0.5)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "]\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    base_estimators.append(\n",
    "        ('xgb', xgb.XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, \n",
    "                                  random_state=42, verbosity=0))\n",
    "    )\n",
    "else:\n",
    "    base_estimators.append(\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=4, \n",
    "                                          learning_rate=0.1, random_state=42))\n",
    "    )\n",
    "\n",
    "# Meta-model (simple Ridge works well)\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=Ridge(alpha=0.1),\n",
    "    cv=5,  # 5-fold CV for out-of-fold predictions\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training stacking ensemble (this may take a moment)...\")\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d03274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate stacking ensemble\n",
    "stack_val_pred = stacking_model.predict(X_val_scaled)\n",
    "stack_test_pred = stacking_model.predict(X_test_scaled)\n",
    "\n",
    "stack_rmse_val = np.sqrt(mean_squared_error(y_val, stack_val_pred))\n",
    "stack_rmse_test = np.sqrt(mean_squared_error(y_test, stack_test_pred))\n",
    "\n",
    "print(f\"Stacking Ensemble:\")\n",
    "print(f\"  Validation RMSE: {stack_rmse_val:.4f}\")\n",
    "print(f\"  Test RMSE:       {stack_rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What weights did the meta-model learn?\n",
    "meta_model = stacking_model.final_estimator_\n",
    "print(\"Meta-model coefficients (learned weights):\")\n",
    "for (name, _), coef in zip(base_estimators, meta_model.coef_):\n",
    "    print(f\"  {name}: {coef:.3f}\")\n",
    "print(f\"  intercept: {meta_model.intercept_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595b3aa",
   "metadata": {},
   "source": [
    "## 6. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "comparison = [\n",
    "    {'Method': 'Best Single Model', 'Val RMSE': best_single['val_rmse'], \n",
    "     'Test RMSE': np.sqrt(mean_squared_error(y_test, test_predictions[best_single['model']]))},\n",
    "    {'Method': 'Simple Average', 'Val RMSE': avg_rmse_val, 'Test RMSE': avg_rmse_test},\n",
    "    {'Method': 'Weighted (Inverse RMSE)', 'Val RMSE': weighted_rmse_val, 'Test RMSE': weighted_rmse_test},\n",
    "]\n",
    "\n",
    "try:\n",
    "    comparison.append({'Method': 'Weighted (Optimized)', 'Val RMSE': optimal_rmse_val, 'Test RMSE': optimal_rmse_test})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "comparison.append({'Method': 'Stacking', 'Val RMSE': stack_rmse_val, 'Test RMSE': stack_rmse_test})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(\"Ensemble Method Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd0adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Val RMSE'], width, label='Validation', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['Test RMSE'], width, label='Test', color='coral')\n",
    "\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Ensemble Method Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Method'], rotation=20, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03254227",
   "metadata": {},
   "source": [
    "## 7. Practical Tips for Hockey Ensembles\n",
    "\n",
    "### Recommended Ensemble\n",
    "\n",
    "For hockey prediction, a simple weighted ensemble often works best:\n",
    "\n",
    "```python\n",
    "# Good starter ensemble\n",
    "models = {\n",
    "    'elo': EloModel(),           # Captures team strength dynamics\n",
    "    'xgb': XGBRegressor(),        # Captures complex patterns\n",
    "    'rf': RandomForestRegressor(), # Robust tree predictions\n",
    "    'ridge': Ridge(),              # Captures linear relationships\n",
    "}\n",
    "```\n",
    "\n",
    "### Do's and Don'ts\n",
    "\n",
    "✅ **Do:**\n",
    "- Use diverse model types (linear + trees + neural)\n",
    "- Validate ensemble weights on held-out data\n",
    "- Start with simple averaging\n",
    "- Include domain-specific models (ELO for sports)\n",
    "\n",
    "❌ **Don't:**\n",
    "- Overfit ensemble weights on validation data\n",
    "- Use too many similar models (3 different RF = just one RF)\n",
    "- Forget to retrain on full data for production\n",
    "- Make ensembles too complex (diminishing returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*50)\n",
    "print(\" ENSEMBLE RECOMMENDATIONS FOR HOCKEY\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. QUICK WIN: Simple average of 3-4 diverse models\n",
    "   - Usually gives 5-15% improvement\n",
    "\n",
    "2. BETTER: Inverse RMSE weighted average\n",
    "   - Easy to implement, no extra training\n",
    "\n",
    "3. BEST: Stacking with Ridge meta-model\n",
    "   - Learns optimal combination automatically\n",
    "   - Use 5-fold CV for out-of-fold predictions\n",
    "\n",
    "RECOMMENDED BASE MODELS:\n",
    "- EloModel (captures rating dynamics)\n",
    "- XGBRegressor (best for complex patterns)\n",
    "- RandomForest (robust, less tuning needed)\n",
    "- Ridge (captures linear relationships)\n",
    "\n",
    "TYPICAL WEIGHTS (goals prediction):\n",
    "- XGBoost: 35-45%\n",
    "- Random Forest: 25-35%\n",
    "- ELO: 15-25%\n",
    "- Linear: 10-15%\n",
    "\"\"\")\n",
    "print(\"Tutorial complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
