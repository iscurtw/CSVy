{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b881f2",
   "metadata": {},
   "source": [
    "# Neural Network Tutorial for Hockey Prediction\n",
    "\n",
    "This tutorial explains how neural networks work and how to use them\n",
    "for predicting hockey game outcomes.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Neural Network Basics** - How MLPs work\n",
    "2. **Why Scaling Matters** - Critical for neural networks!\n",
    "3. **Architecture Design** - Layers and neurons\n",
    "4. **Key Hyperparameters** - Learning rate, regularization\n",
    "5. **Training Dynamics** - Loss curves and early stopping\n",
    "6. **Practical Usage** - Training and prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384b29c",
   "metadata": {},
   "source": [
    "## 1. Understanding Neural Networks (MLPs)\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a series of **layers** that transform\n",
    "input features into predictions.\n",
    "\n",
    "### Layer Computation\n",
    "\n",
    "$$h^{(l)} = \\sigma(W^{(l)} h^{(l-1)} + b^{(l)})$$\n",
    "\n",
    "Where:\n",
    "- $h^{(l)}$ = output of layer $l$\n",
    "- $W^{(l)}$ = weight matrix (learned)\n",
    "- $b^{(l)}$ = bias vector (learned)\n",
    "- $\\sigma$ = activation function (introduces non-linearity)\n",
    "\n",
    "### Architecture Example\n",
    "```\n",
    "Input (7 features)\n",
    "    ↓\n",
    "Hidden Layer 1 (100 neurons, ReLU)\n",
    "    ↓\n",
    "Hidden Layer 2 (50 neurons, ReLU)\n",
    "    ↓\n",
    "Output (1 neuron, linear)\n",
    "```\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "| Function | Formula | When to Use |\n",
    "|----------|---------|-------------|\n",
    "| ReLU | max(0, x) | Default for hidden layers |\n",
    "| Tanh | (e^x - e^-x)/(e^x + e^-x) | When inputs are centered |\n",
    "| Sigmoid | 1/(1 + e^-x) | Binary outputs (0-1) |\n",
    "| Linear | x | Output layer for regression |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15435a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Tutorial ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample hockey data\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'elo_diff': np.random.normal(0, 100, n),\n",
    "    'home_win_pct': np.random.uniform(0.35, 0.65, n),\n",
    "    'away_win_pct': np.random.uniform(0.35, 0.65, n),\n",
    "    'home_goals_avg': np.random.uniform(2.5, 3.5, n),\n",
    "    'away_goals_against': np.random.uniform(2.5, 3.5, n),\n",
    "    'home_pp_pct': np.random.uniform(0.15, 0.25, n),\n",
    "    'rest_days': np.random.choice([1, 2, 3, 4], n),\n",
    "})\n",
    "\n",
    "# Target with non-linear relationships\n",
    "data['home_goals'] = (\n",
    "    2.5 +\n",
    "    0.005 * data['elo_diff'] +\n",
    "    np.where(data['elo_diff'] > 50, 0.4, 0) +\n",
    "    0.8 * data['home_win_pct'] +\n",
    "    4 * data['home_pp_pct'] +\n",
    "    0.3 * np.sin(data['rest_days']) +  # Non-linear pattern\n",
    "    np.random.normal(0, 0.5, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"\\nFeature ranges (note: very different scales!):\")\n",
    "print(data.describe().loc[['min', 'max', 'mean', 'std']].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52557d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "feature_cols = [c for c in data.columns if c != 'home_goals']\n",
    "X = data[feature_cols]\n",
    "y = data['home_goals']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Training: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1959e51",
   "metadata": {},
   "source": [
    "## 2. Why Scaling is CRITICAL\n",
    "\n",
    "Neural networks are **extremely sensitive** to feature scales!\n",
    "\n",
    "### The Problem\n",
    "\n",
    "- `elo_diff` ranges from -300 to +300\n",
    "- `home_pp_pct` ranges from 0.15 to 0.25\n",
    "\n",
    "Without scaling, the network focuses on the large-scale feature\n",
    "and ignores the small-scale one (even if small-scale is important!).\n",
    "\n",
    "### The Solution: StandardScaler\n",
    "\n",
    "$$x_{scaled} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "This makes all features have:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7328f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Training without scaling\n",
    "model_unscaled = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "model_unscaled.fit(X_train, y_train)\n",
    "unscaled_rmse = np.sqrt(mean_squared_error(y_test, model_unscaled.predict(X_test)))\n",
    "\n",
    "print(f\"Without scaling: RMSE = {unscaled_rmse:.4f}\")\n",
    "print(f\"(Iterations used: {model_unscaled.n_iter_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d44d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIGHT: Scale the data first\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"After scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.4f}\")\n",
    "print(f\"  Std:  {X_train_scaled.std():.4f}\")\n",
    "\n",
    "model_scaled = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "scaled_rmse = np.sqrt(mean_squared_error(y_test, model_scaled.predict(X_test_scaled)))\n",
    "\n",
    "print(f\"\\nWith scaling: RMSE = {scaled_rmse:.4f}\")\n",
    "print(f\"(Iterations used: {model_scaled.n_iter_})\")\n",
    "\n",
    "improvement = (unscaled_rmse - scaled_rmse) / unscaled_rmse * 100\n",
    "print(f\"\\nImprovement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea40d5c",
   "metadata": {},
   "source": [
    "## 3. Architecture Design\n",
    "\n",
    "### How Many Layers?\n",
    "\n",
    "| Layers | Use Case |\n",
    "|--------|----------|\n",
    "| 1 | Simple patterns, small data |\n",
    "| 2 | Most tabular data (recommended) |\n",
    "| 3+ | Complex patterns, lots of data |\n",
    "\n",
    "### How Many Neurons?\n",
    "\n",
    "Rules of thumb:\n",
    "- Start with 2× number of input features\n",
    "- Each layer can be smaller than the previous\n",
    "- Common pattern: (100, 50) or (64, 32)\n",
    "\n",
    "### For Hockey (7-15 features):\n",
    "- Good: `(50,)` - single layer\n",
    "- Better: `(100, 50)` - two layers\n",
    "- Overkill: `(200, 100, 50)` - too deep for simple data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd77705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare architectures\n",
    "architectures = [\n",
    "    ((50,), 'Shallow (50)'),\n",
    "    ((100,), 'Wide (100)'),\n",
    "    ((100, 50), 'Standard (100, 50)'),\n",
    "    ((100, 50, 25), 'Deep (100, 50, 25)'),\n",
    "]\n",
    "\n",
    "arch_results = []\n",
    "for layers, name in architectures:\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=layers,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, model.predict(X_train_scaled)))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test_scaled)))\n",
    "    \n",
    "    # Count parameters\n",
    "    n_params = sum(w.size for w in model.coefs_) + sum(b.size for b in model.intercepts_)\n",
    "    \n",
    "    arch_results.append({\n",
    "        'Architecture': name,\n",
    "        'Params': n_params,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Iterations': model.n_iter_\n",
    "    })\n",
    "\n",
    "arch_df = pd.DataFrame(arch_results)\n",
    "print(\"Architecture Comparison:\")\n",
    "print(arch_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94139c0",
   "metadata": {},
   "source": [
    "## 4. Key Hyperparameters\n",
    "\n",
    "### Learning Rate (`learning_rate_init`)\n",
    "- Controls step size during training\n",
    "- Too high: unstable training, may not converge\n",
    "- Too low: very slow training\n",
    "- Default: 0.001 (usually good)\n",
    "\n",
    "### Regularization (`alpha`)\n",
    "- L2 penalty on weights\n",
    "- Higher = simpler model (less overfitting)\n",
    "- Lower = more flexible (risk of overfitting)\n",
    "- Try: 0.0001, 0.001, 0.01, 0.1\n",
    "\n",
    "### Batch Size (`batch_size`)\n",
    "- Samples per gradient update\n",
    "- Smaller = noisier updates, may escape local minima\n",
    "- Larger = smoother updates, faster convergence\n",
    "- Default: 'auto' (min(200, n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning rates\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test_scaled)))\n",
    "    \n",
    "    lr_results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Iterations': model.n_iter_,\n",
    "        'Final Loss': model.loss_\n",
    "    })\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "print(\"Learning Rate Comparison:\")\n",
    "print(lr_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization (alpha)\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "alpha_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        alpha=alpha,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, model.predict(X_train_scaled)))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test_scaled)))\n",
    "    \n",
    "    alpha_results.append({\n",
    "        'Alpha': alpha,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Gap': test_rmse - train_rmse\n",
    "    })\n",
    "\n",
    "alpha_df = pd.DataFrame(alpha_results)\n",
    "print(\"Regularization (Alpha) Comparison:\")\n",
    "print(alpha_df.to_string(index=False))\n",
    "print(\"\\nNote: Large gap = overfitting. Higher alpha reduces gap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5fdcc5",
   "metadata": {},
   "source": [
    "## 5. Training Dynamics\n",
    "\n",
    "### Loss Curve\n",
    "Shows how training loss decreases over iterations.\n",
    "- Should decrease smoothly\n",
    "- If jagged: learning rate too high\n",
    "- If plateaus early: learning rate too low or stuck\n",
    "\n",
    "### Early Stopping\n",
    "Stop training when validation error stops improving.\n",
    "Prevents overfitting without manually tuning iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eceb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and plot loss curve\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(model.loss_curve_, color='steelblue', linewidth=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss Curve')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark convergence\n",
    "ax.axvline(model.n_iter_, color='red', linestyle='--', \n",
    "           label=f'Converged at iter {model.n_iter_}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "model_es = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    early_stopping=True,       # Enable early stopping\n",
    "    validation_fraction=0.1,   # 10% of training for validation\n",
    "    n_iter_no_change=10,       # Stop if no improvement for 10 iters\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "model_es.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Early Stopping:\")\n",
    "print(f\"  Stopped at iteration: {model_es.n_iter_}\")\n",
    "print(f\"  Best validation score: {model_es.best_validation_score_:.4f}\")\n",
    "\n",
    "# Plot validation curve\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(model_es.loss_curve_, label='Training Loss', color='steelblue')\n",
    "ax.plot(model_es.validation_scores_, label='Validation Score (R²)', color='coral')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Training with Early Stopping')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75e8c8",
   "metadata": {},
   "source": [
    "## 6. Practical Usage: Goal Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23961dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model with good defaults\n",
    "final_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "    activation='relu',             # Standard activation\n",
    "    solver='adam',                 # Best optimizer for most cases\n",
    "    alpha=0.01,                    # Some regularization\n",
    "    learning_rate_init=0.001,      # Default learning rate\n",
    "    early_stopping=True,           # Prevent overfitting\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=15,\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Remember: always scale!\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "test_pred = final_model.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "mae = mean_absolute_error(y_test, test_pred)\n",
    "r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "print(f\"Final Model Performance:\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "print(f\"  R²:   {r2:.4f}\")\n",
    "print(f\"  Iterations: {final_model.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Save both model and scaler!\n",
    "import pickle\n",
    "\n",
    "# In production, you need both:\n",
    "# 1. The scaler (to transform new data)\n",
    "# 2. The model (to make predictions)\n",
    "\n",
    "# Example save\n",
    "# pickle.dump({'model': final_model, 'scaler': scaler}, open('nn_model.pkl', 'wb'))\n",
    "\n",
    "print(\"Remember: Always save the scaler with the model!\")\n",
    "print(\"New predictions require: scaler.transform(new_data) → model.predict()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for a new game\n",
    "new_game = pd.DataFrame([{\n",
    "    'elo_diff': 100,\n",
    "    'home_win_pct': 0.55,\n",
    "    'away_win_pct': 0.45,\n",
    "    'home_goals_avg': 3.2,\n",
    "    'away_goals_against': 3.0,\n",
    "    'home_pp_pct': 0.22,\n",
    "    'rest_days': 2,\n",
    "}])\n",
    "\n",
    "# Scale first!\n",
    "new_game_scaled = scaler.transform(new_game)\n",
    "predicted_goals = final_model.predict(new_game_scaled)[0]\n",
    "\n",
    "print(f\"Predicted home goals: {predicted_goals:.2f}\")\n",
    "print(f\"Rounded: {round(predicted_goals)} goals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277927a0",
   "metadata": {},
   "source": [
    "## 7. Neural Networks vs Other Models\n",
    "\n",
    "### When to Use Neural Networks\n",
    "\n",
    "✅ **Good for:**\n",
    "- Complex non-linear patterns\n",
    "- Large datasets (1000+ samples)\n",
    "- Many features (can learn interactions)\n",
    "- When you have time to tune\n",
    "\n",
    "❌ **Not great for:**\n",
    "- Small datasets (trees often better)\n",
    "- Interpretability (black box)\n",
    "- Quick prototypes (need scaling, tuning)\n",
    "- Tabular data with clear structure (XGBoost often wins)\n",
    "\n",
    "### For Hockey Predictions:\n",
    "\n",
    "Neural networks are **useful in ensembles** because they capture\n",
    "different patterns than tree-based models. But XGBoost/RF often\n",
    "perform as well or better on typical hockey data.\n",
    "\n",
    "**Recommendation:** Include MLP in your ensemble, but don't rely on it alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a41e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with other models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "comparison = []\n",
    "\n",
    "# Ridge (needs scaling)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "comparison.append({'Model': 'Ridge', \n",
    "                  'RMSE': np.sqrt(mean_squared_error(y_test, ridge.predict(X_test_scaled)))})\n",
    "\n",
    "# Random Forest (no scaling needed)\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "comparison.append({'Model': 'Random Forest',\n",
    "                  'RMSE': np.sqrt(mean_squared_error(y_test, rf.predict(X_test)))})\n",
    "\n",
    "# Gradient Boosting (no scaling needed)\n",
    "gb = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "comparison.append({'Model': 'Gradient Boosting',\n",
    "                  'RMSE': np.sqrt(mean_squared_error(y_test, gb.predict(X_test)))})\n",
    "\n",
    "# Neural Network (needs scaling)\n",
    "comparison.append({'Model': 'Neural Network', 'RMSE': rmse})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison).sort_values('RMSE')\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ece17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*50)\n",
    "print(\" NEURAL NETWORK RECOMMENDATIONS FOR HOCKEY\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "CRITICAL: Always scale your features!\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "RECOMMENDED PARAMETERS:\n",
    "params = {\n",
    "    'hidden_layer_sizes': (100, 50),  # 2 layers\n",
    "    'activation': 'relu',              # Standard\n",
    "    'solver': 'adam',                  # Best optimizer\n",
    "    'alpha': 0.01,                     # Regularization\n",
    "    'learning_rate_init': 0.001,       # Default LR\n",
    "    'early_stopping': True,            # Prevents overfit\n",
    "    'validation_fraction': 0.1,\n",
    "    'n_iter_no_change': 15,\n",
    "    'max_iter': 500,\n",
    "}\n",
    "\n",
    "KEY POINTS:\n",
    "1. Scale data with StandardScaler\n",
    "2. Save scaler with model\n",
    "3. Use early stopping\n",
    "4. 2 hidden layers is usually enough\n",
    "5. Include in ensembles for diversity\n",
    "\"\"\")\n",
    "print(\"Tutorial complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
