{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ed1021",
   "metadata": {},
   "source": [
    "# XGBoost Tutorial for Hockey Prediction\n",
    "\n",
    "This tutorial explains how XGBoost works and how to use it for\n",
    "predicting hockey game outcomes.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Gradient Boosting Basics** - How boosted trees work\n",
    "2. **Key Hyperparameters** - Learning rate, depth, regularization\n",
    "3. **Feature Importance** - Understanding what features matter\n",
    "4. **Overfitting Prevention** - Early stopping and regularization\n",
    "5. **Practical Usage** - Training and prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629d26d",
   "metadata": {},
   "source": [
    "## 1. Understanding Gradient Boosting\n",
    "\n",
    "XGBoost builds a prediction by **adding trees sequentially**, where each\n",
    "new tree corrects the errors of the previous ones.\n",
    "\n",
    "$$\\hat{y} = \\sum_{k=1}^{K} f_k(x)$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ = final prediction\n",
    "- $K$ = number of trees (n_estimators)\n",
    "- $f_k$ = individual tree prediction\n",
    "\n",
    "### Why \"Gradient\" Boosting?\n",
    "\n",
    "Each tree is trained on the **gradient** (direction of steepest error reduction):\n",
    "\n",
    "1. Make initial prediction (e.g., average goals)\n",
    "2. Calculate errors (residuals)\n",
    "3. Train a tree to predict the errors\n",
    "4. Add tree prediction × learning_rate to running total\n",
    "5. Repeat\n",
    "\n",
    "### Example Flow\n",
    "```\n",
    "Game: Team A (ELO 1600) vs Team B (ELO 1400)\n",
    "Step 1: Initial prediction = 3.0 goals (league average)\n",
    "Step 2: Actual = 5 goals, Error = +2\n",
    "Step 3: Tree 1 learns \"high ELO diff → more goals\"\n",
    "Step 4: New prediction = 3.0 + 0.1 × 1.5 = 3.15\n",
    "... continue until error is minimized\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Check if XGBoost is available\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not installed. Run: pip install xgboost\")\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    print(\"Using sklearn's GradientBoostingRegressor as fallback\")\n",
    "\n",
    "print(\"Tutorial ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample hockey data\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'elo_diff': np.random.normal(0, 100, n),\n",
    "    'home_win_pct': np.random.uniform(0.35, 0.65, n),\n",
    "    'away_win_pct': np.random.uniform(0.35, 0.65, n),\n",
    "    'home_goals_avg': np.random.uniform(2.5, 3.5, n),\n",
    "    'away_goals_against': np.random.uniform(2.5, 3.5, n),\n",
    "    'home_pp_pct': np.random.uniform(0.15, 0.25, n),\n",
    "    'rest_days': np.random.choice([1, 2, 3, 4], n),\n",
    "})\n",
    "\n",
    "# Non-linear target (XGBoost shines here!)\n",
    "data['home_goals'] = (\n",
    "    2.5 +\n",
    "    0.005 * data['elo_diff'] +\n",
    "    np.where(data['elo_diff'] > 50, 0.5, 0) +  # Non-linear: bonus for big advantage\n",
    "    0.8 * data['home_win_pct'] +\n",
    "    0.3 * (data['home_goals_avg'] - data['away_goals_against']) +\n",
    "    5 * data['home_pp_pct'] +\n",
    "    np.where(data['rest_days'] >= 3, 0.2, 0) +  # Non-linear: rested teams score more\n",
    "    np.random.normal(0, 0.5, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Goals distribution: {data['home_goals'].value_counts().sort_index().to_dict()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63dac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "feature_cols = [c for c in data.columns if c != 'home_goals']\n",
    "X = data[feature_cols]\n",
    "y = data['home_goals']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Training: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53a9fa",
   "metadata": {},
   "source": [
    "## 2. Key Hyperparameters\n",
    "\n",
    "XGBoost has many parameters. Here are the most important:\n",
    "\n",
    "| Parameter | Range | Effect |\n",
    "|-----------|-------|--------|\n",
    "| `n_estimators` | 50-500 | More trees = more capacity (risk overfitting) |\n",
    "| `learning_rate` | 0.01-0.3 | Lower = more trees needed but better generalization |\n",
    "| `max_depth` | 3-10 | Deeper trees capture more complex patterns |\n",
    "| `min_child_weight` | 1-10 | Higher = more conservative (prevents overfitting) |\n",
    "| `subsample` | 0.6-1.0 | Fraction of data used per tree |\n",
    "| `colsample_bytree` | 0.6-1.0 | Fraction of features per tree |\n",
    "| `reg_alpha` | 0-1 | L1 regularization (sparsity) |\n",
    "| `reg_lambda` | 0-1 | L2 regularization (smoothness) |\n",
    "\n",
    "### Golden Rule\n",
    "**Lower learning_rate + More n_estimators = Better results** (but slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning rates\n",
    "learning_rates = [0.01, 0.1, 0.3]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    if XGB_AVAILABLE:\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=lr,\n",
    "            max_depth=4,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )\n",
    "    else:\n",
    "        model = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=lr,\n",
    "            max_depth=4,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Learning Rate Comparison (100 trees):\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nNote: Lower LR often needs more trees to converge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tree depth\n",
    "depths = [2, 4, 6, 8]\n",
    "depth_results = []\n",
    "\n",
    "for depth in depths:\n",
    "    if XGB_AVAILABLE:\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=depth,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )\n",
    "    else:\n",
    "        model = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=depth,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    depth_results.append({\n",
    "        'max_depth': depth,\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "        'gap': np.sqrt(mean_squared_error(y_train, train_pred)) - \n",
    "               np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    })\n",
    "\n",
    "depth_df = pd.DataFrame(depth_results)\n",
    "print(\"Tree Depth Comparison:\")\n",
    "print(depth_df.to_string(index=False))\n",
    "print(\"\\nNote: Large gap = overfitting. Depth 4-6 is often optimal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c272e5",
   "metadata": {},
   "source": [
    "## 3. Feature Importance\n",
    "\n",
    "XGBoost provides multiple importance metrics:\n",
    "\n",
    "- **weight**: Number of times feature is used in splits\n",
    "- **gain**: Average improvement when feature is used\n",
    "- **cover**: Average number of samples affected\n",
    "\n",
    "**Gain is usually most informative** for understanding predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for feature importance\n",
    "if XGB_AVAILABLE:\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "else:\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance)))\n",
    "ax.barh(importance['feature'], importance['importance'], color=colors)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('XGBoost Feature Importance')\n",
    "ax.invert_yaxis()  # Highest on top\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd23aa",
   "metadata": {},
   "source": [
    "## 4. Preventing Overfitting\n",
    "\n",
    "XGBoost can easily overfit. Key strategies:\n",
    "\n",
    "### 4.1 Early Stopping\n",
    "Stop training when validation error stops improving.\n",
    "\n",
    "### 4.2 Regularization\n",
    "- `reg_alpha` (L1): Pushes weights toward zero\n",
    "- `reg_lambda` (L2): Penalizes large weights\n",
    "\n",
    "### 4.3 Subsampling\n",
    "- `subsample`: Use random 70-90% of data per tree\n",
    "- `colsample_bytree`: Use random 70-90% of features per tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping example\n",
    "if XGB_AVAILABLE:\n",
    "    # Split training into train/validation for early stopping\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    model_es = xgb.XGBRegressor(\n",
    "        n_estimators=500,  # Set high, early stopping will find optimal\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        early_stopping_rounds=20,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    model_es.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Early stopping: Best iteration = {model_es.best_iteration}\")\n",
    "    print(f\"(Out of 500 max trees, only {model_es.best_iteration} were needed)\")\n",
    "else:\n",
    "    print(\"Early stopping requires XGBoost library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization\n",
    "reg_configs = [\n",
    "    {'name': 'No regularization', 'reg_alpha': 0, 'reg_lambda': 0},\n",
    "    {'name': 'L1 only', 'reg_alpha': 1, 'reg_lambda': 0},\n",
    "    {'name': 'L2 only', 'reg_alpha': 0, 'reg_lambda': 1},\n",
    "    {'name': 'Both', 'reg_alpha': 0.5, 'reg_lambda': 0.5},\n",
    "]\n",
    "\n",
    "reg_results = []\n",
    "for config in reg_configs:\n",
    "    if XGB_AVAILABLE:\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            reg_alpha=config['reg_alpha'],\n",
    "            reg_lambda=config['reg_lambda'],\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )\n",
    "    else:\n",
    "        # sklearn doesn't have same regularization params\n",
    "        model = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, model.predict(X_train)))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test)))\n",
    "    \n",
    "    reg_results.append({\n",
    "        'config': config['name'],\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'overfit_gap': train_rmse - test_rmse\n",
    "    })\n",
    "\n",
    "reg_df = pd.DataFrame(reg_results)\n",
    "print(\"Regularization Comparison:\")\n",
    "print(reg_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831813e2",
   "metadata": {},
   "source": [
    "## 5. Practical Usage: Goal Predictor\n",
    "\n",
    "Let's build a complete goal predictor with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model with good defaults\n",
    "if XGB_AVAILABLE:\n",
    "    final_model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "else:\n",
    "    final_model = GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        min_samples_leaf=3,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "test_pred = final_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "mae = mean_absolute_error(y_test, test_pred)\n",
    "\n",
    "print(f\"Final Model Performance:\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  MAE:  {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for a new game\n",
    "new_game = pd.DataFrame([{\n",
    "    'elo_diff': 100,           # Home team 100 ELO higher\n",
    "    'home_win_pct': 0.55,      # Home team wins 55%\n",
    "    'away_win_pct': 0.45,      # Away team wins 45%\n",
    "    'home_goals_avg': 3.2,     # Home averages 3.2 goals\n",
    "    'away_goals_against': 3.0, # Away allows 3.0 goals\n",
    "    'home_pp_pct': 0.22,       # 22% power play\n",
    "    'rest_days': 2,            # 2 days rest\n",
    "}])\n",
    "\n",
    "predicted_goals = final_model.predict(new_game)[0]\n",
    "print(f\"Predicted home goals: {predicted_goals:.2f}\")\n",
    "print(f\"Rounded: {round(predicted_goals)} goals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5920eeb",
   "metadata": {},
   "source": [
    "## 6. Tips for Hockey Predictions\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start simple**: `max_depth=4`, `learning_rate=0.1`, `n_estimators=100`\n",
    "2. **Use early stopping**: Prevents overfitting automatically\n",
    "3. **Feature engineering matters**: ELO diff, recent form, rest days are key\n",
    "4. **Regularize**: Always use `subsample=0.8` and some L2 regularization\n",
    "5. **Cross-validate**: Use 5-fold CV to estimate true performance\n",
    "\n",
    "### Hyperparameter Tuning Order\n",
    "\n",
    "1. `n_estimators` + `learning_rate` (use early stopping)\n",
    "2. `max_depth` + `min_child_weight`\n",
    "3. `subsample` + `colsample_bytree`\n",
    "4. `reg_alpha` + `reg_lambda`\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "❌ Using too many trees without early stopping  \n",
    "❌ Setting max_depth too high (>6 usually overfits)  \n",
    "❌ Ignoring feature importance (use it to prune bad features)  \n",
    "❌ Not scaling data... wait, XGBoost doesn't need scaling! ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f519fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of recommended parameters\n",
    "print(\"=\"*50)\n",
    "print(\" RECOMMENDED XGBOOST PARAMETERS FOR HOCKEY\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "params = {\n",
    "    'n_estimators': 200,      # Use early stopping\n",
    "    'learning_rate': 0.05,    # Lower = better, but slower\n",
    "    'max_depth': 4,           # 3-5 for hockey data\n",
    "    'min_child_weight': 3,    # Prevents tiny leaf nodes\n",
    "    'subsample': 0.8,         # Random 80% of data\n",
    "    'colsample_bytree': 0.8,  # Random 80% of features\n",
    "    'reg_alpha': 0.1,         # L1 regularization\n",
    "    'reg_lambda': 0.1,        # L2 regularization\n",
    "}\n",
    "\"\"\")\n",
    "print(\"Tutorial complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
